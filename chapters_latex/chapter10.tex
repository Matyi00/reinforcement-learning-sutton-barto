\section*{Chapter 10}

\subsection*{Exercise 10.1}

We have not explicitly considered or given pseudocode for any Monte Carlo
methods in this chapter. What would they be like? Why is it reasonable not to give
pseudocode for them? How would they perform on the Mountain Car task?
\subsection*{Solution}

Monte Carlo methods are basically n-step methods with $n$ equal to the length of the episode  ($T$).
The pseudocode doesn't need many changes from the n-step methods, the only difference is that $n$ is always equal to $T$.

Based on Figure 10.4, we can see that the agent behaves worse with the increase of $n$, so Monte Carlo methods would perform poorly on the task.

\subsection*{Exercise 10.2}

Give pseudocode for semi-gradient one-step Expected Sarsa for control.

\subsection*{Solution}

\fbox{
\begin{minipage}{0.95\textwidth}
\footnotesize 
    Input: a differentiable action-value function parameterization $\hat{q} : \mathcal{S} \times \mathcal{A} \times \mathbb{R}^d \to \mathbb{R}$

    Algorithm parameters: step size $\alpha > 0$, small $\epsilon > 0$

    Initialize value-function weights $\mathbf{w} \in \mathbb{R}^d$ arbitrarily (e.g., $\mathbf{w} = \mathbf{0}$)


    Loop for each episode:
    \begin{itemize}[left=0em]
        \item[] $S, A \leftarrow$ initial state and action of episode (e.g., $\epsilon$-greedy)
        \item[] Loop for each step of episode:
        \begin{itemize}[left=0em]
            \item[] Take action $A$, observe $R, S'$
            \item[] If $S'$ is terminal:
            \begin{itemize}[left=0em]
                \item[] $\mathbf{w} \leftarrow \mathbf{w} + \alpha \left[ R - \hat{q}(S, A, \mathbf{w}) \right] \nabla \hat{q}(S, A, \mathbf{w})$
                \item[] Go to next episode
            \end{itemize}
            \item[] Choose $A'$ as a function of $\hat{q}(S', \cdot, \mathbf{w})$ (e.g., $\epsilon$-greedy)
            \item[] \textcolor{blue}{$\mathbf{w} \leftarrow \mathbf{w} + \alpha \left[ R + \gamma \sum_a \pi(a | S') \hat{q}(S', a, \mathbf{w}) - \hat{q}(S, A, \mathbf{w}) \right] \nabla \hat{q}(S, A, \mathbf{w})$}
            \item[] $S \leftarrow S'$
            \item[] $A \leftarrow A'$
        \end{itemize}
    \end{itemize}
\end{minipage}
}


\subsection*{Exercise 10.3}

Why do the results shown in Figure 10.4 have higher standard errors at
large $n$ than at small $n$?

\subsection*{Solution}

The higher standard errors at large $n$ arise due to the accumulation of errors over multiple steps (with regard to action selection) and delayed updates. 

\subsection*{Exercise 10.4}

Give pseudocode for a differential version of semi-gradient Q-learning.

\subsection*{Solution}

\fbox{
\begin{minipage}{0.95\textwidth}
\footnotesize 
    Input: a differentiable action-value function parameterization $\hat{q} : \mathcal{S} \times \mathcal{A} \times \mathbb{R}^d \to \mathbb{R}$

    Algorithm parameters: step sizes $\alpha, \beta > 0$, small $\epsilon > 0$

    Initialize value-function weights $\mathbf{w} \in \mathbb{R}^d$ arbitrarily (e.g., $\mathbf{w} = \mathbf{0}$)

    Initialize average reward estimate $\bar{R} \in \mathbb{R}$ arbitrarily (e.g., $\bar{R} = 0$)
    

    \textcolor{blue}{Initialize state $S$}

    Loop for each step:
    \begin{itemize}[left=0em]
        \item[] \textcolor{blue}{Choose $A$ as a function of $\hat{q}(S, \cdot, \mathbf{w})$ (e.g., $\epsilon$-greedy)}
        \item[] Take action $A$, observe $R, S'$
        \item[] \textcolor{blue}{$A' \leftarrow \argmax_{a'} \hat{q}(S', a', \mathbf{w})$}
        \item[] $\delta \leftarrow R - \bar{R} + \hat{q}(S', A', \mathbf{w}) - \hat{q}(S, A, \mathbf{w})$
        \item[] $\bar{R} \leftarrow \bar{R} + \beta \delta$
        \item[] $\mathbf{w} \leftarrow \mathbf{w} + \alpha \delta \nabla \hat{q}(S, A, \mathbf{w})$
        \item[] $S \leftarrow S'$
    \end{itemize}
\end{minipage}
}

\subsection*{Exercise 10.5}
What equations are needed (beyond 10.10) to specify the differential version of TD(0)?

\subsection*{Solution}

\[
    \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \delta_t \nabla \hat{v}(S_t, \mathbf{w}_t)
\]

\subsection*{Exercise 10.6}

Suppose there is an MDP that under any policy produces the deterministic
sequence of rewards $+1$, $0$, $+1$, $0$, $+1$, $0$, $\dots$ going on forever. Technically, this violates
ergodicity; there is no stationary limiting distribution $\mu_\pi$ and the limit (10.7) does not
exist. Nevertheless, the average reward (10.6) is well defined. What is it? Now consider
two states in this MDP. From $\mathsf{A}$, the reward sequence is exactly as described above,
starting with a $+1$, whereas, from $\mathsf{B}$, the reward sequence starts with a $0$ and then
continues with $+1$, $0$, $+1$, $0$, $\dots$. We would like to compute the differential values of $\mathsf{A}$ and
$\mathsf{B}$. Unfortunately, the differential return (10.9) is not well defined when starting from
these states as the implicit limit does not exist. To repair this, one could alternatively
define the differential value of a state as
\[
    v_\pi(s) \doteq \lim_{\gamma \rightarrow 1} \lim_{h \rightarrow \infty} \sum_{t=0}^{h} \gamma^t \left( \mathbb{E} \left[ R_{t+1} \mid S_0 = s \right] - r(\pi) \right).
\]
Under this definition, what are the differential values of states $\mathsf{A}$ and $\mathsf{B}$?

\subsection*{Solution}
\[
    \lim_{h \rightarrow \infty} \frac{1}{h} \sum_{t=1}^{h} \mathbb{E} \left[ R_{t} \mid S_0, A_{0:t-1} \sim  \pi \right] = \lim_{h \rightarrow \infty} \frac{1}{h}  \left\lfloor \frac{h+1}{2} \right\rfloor = \frac{1}{2}
\]

Differential value of state $\mathsf{A}$:
\begin{align*}
    \lim_{\gamma \rightarrow 1} \lim_{h \rightarrow \infty} \sum_{t=0}^{h} \gamma^t \left( \mathbb{E} \left[ R_{t+1} \mid S_0 = \mathsf{A} \right] - r(\pi) \right) &=  \lim_{\gamma \rightarrow 1} \lim_{h \rightarrow \infty} \sum_{t=0}^{h} \frac{1}{2}\left(-\gamma\right)^t \\
    &= \frac{1}{2}\lim_{\gamma \rightarrow 1} \frac{1}{1 + \gamma} \\
    &= \frac{1}{4}
\end{align*}

Differential value of state $\mathsf{B}$:
\begin{align*}
    \lim_{\gamma \rightarrow 1} \lim_{h \rightarrow \infty} \sum_{t=0}^{h} \gamma^t \left( \mathbb{E} \left[ R_{t+1} \mid S_0 = \mathsf{B} \right] - r(\pi) \right) &=  \lim_{\gamma \rightarrow 1} \lim_{h \rightarrow \infty} \sum_{t=0}^{h} -\frac{1}{2}\left(-\gamma\right)^t \\
    &= -\frac{1}{2}\lim_{\gamma \rightarrow 1} \frac{1}{1 + \gamma} \\
    &= -\frac{1}{4}
\end{align*}

\subsection*{Exercise 10.7}

Consider a Markov reward process consisting of a ring of three states $\mathsf{A}$, $\mathsf{B}$,
and $\mathsf{C}$, with state transitions going deterministically around the ring. A reward of +1 is
received upon arrival in $\mathsf{A}$ and otherwise the reward is 0. What are the differential values
of the three states, using (10.13)? 

\subsection*{Solution}
The average reward is $r(\pi)=\frac{1}{3}$.

Differential value of state $\mathsf{A}$:
\begin{align*}
    \lim_{\gamma \rightarrow 1} \lim_{h \rightarrow \infty} \sum_{t=0}^{h} \gamma^t \left( \mathbb{E} \left[ R_{t+1} \mid S_0 = \mathsf{A} \right] - r(\pi) \right) &=  \lim_{\gamma \rightarrow 1} \lim_{h \rightarrow \infty} \sum_{t=0}^{h} -\frac{1}{3} \gamma^{3t} -\frac{1}{3} \gamma^{3t+1}  + \frac{2}{3} \gamma^{3t+2} \\
    &= \lim_{\gamma \rightarrow 1} \lim_{h \rightarrow \infty} \sum_{t=0}^{h} (\gamma^3)^t \frac{2\gamma^2 - \gamma - 1}{3} \\
    &= \lim_{\gamma \rightarrow 1} \frac{2\gamma^2 - \gamma - 1}{3 - 3\gamma^3} \\
    &= \lim_{\gamma \rightarrow 1} \frac{4\gamma - 1}{-9\gamma^2} \\
    &= - \frac{1}{3}
\end{align*}

Differential value of state $\mathsf{B}$:
\begin{align*}
    \lim_{\gamma \rightarrow 1} \lim_{h \rightarrow \infty} \sum_{t=0}^{h} \gamma^t \left( \mathbb{E} \left[ R_{t+1} \mid S_0 = \mathsf{B} \right] - r(\pi) \right) &=  \lim_{\gamma \rightarrow 1} \lim_{h \rightarrow \infty} \sum_{t=0}^{h} -\frac{1}{3} \gamma^{3t} +\frac{2}{3} \gamma^{3t+1}  - \frac{1}{3} \gamma^{3t+2} \\
    &= \lim_{\gamma \rightarrow 1} \lim_{h \rightarrow \infty} \sum_{t=0}^{h} (\gamma^3)^t \frac{-\gamma^2 + 2\gamma - 1}{3} \\
    &= \lim_{\gamma \rightarrow 1} \frac{-\gamma^2 + 2\gamma - 1}{3 - 3\gamma^3} \\
    &= \lim_{\gamma \rightarrow 1} \frac{-2\gamma + 2}{-9\gamma^2} \\
    &= 0
\end{align*}

Differential value of state $\mathsf{C}$:
\begin{align*}
    \lim_{\gamma \rightarrow 1} \lim_{h \rightarrow \infty} \sum_{t=0}^{h} \gamma^t \left( \mathbb{E} \left[ R_{t+1} \mid S_0 = \mathsf{C} \right] - r(\pi) \right) &=  \lim_{\gamma \rightarrow 1} \lim_{h \rightarrow \infty} \sum_{t=0}^{h} \frac{2}{3} \gamma^{3t} -\frac{1}{3} \gamma^{3t+1}  - \frac{1}{3} \gamma^{3t+2} \\
    &= \lim_{\gamma \rightarrow 1} \lim_{h \rightarrow \infty} \sum_{t=0}^{h} (\gamma^3)^t \frac{-\gamma^2 - \gamma + 2}{3} \\
    &= \lim_{\gamma \rightarrow 1} \frac{-\gamma^2 -\gamma + 2}{3 - 3\gamma^3} \\
    &= \lim_{\gamma \rightarrow 1} \frac{-2\gamma - 1}{-9\gamma^2} \\
    &= \frac{1}{3}
\end{align*}

\subsection*{Exercise 10.8}

The pseudocode in the box on page 251 updates $\bar{R}_t$ using  $\delta_t$ as an error
rather than simply $R_{t+1} - \bar{R}_t$. Both errors work, but using $\delta_t$ is better. To see why,
consider the ring MRP of three states from Exercise 10.7. The estimate of the average
reward should tend towards its true value of $\frac{1}{3}$. Suppose it was already there and was
held stuck there. What would the sequence of $R_{t+1} - \bar{R}_t$ errors be? What would the
sequence of $\delta_t$ errors be (using Equation 10.10)? Which error sequence would produce
a more stable estimate of the average reward if the estimate were allowed to change in
response to the errors? Why?

\subsection*{Solution}

Sequence of $R_{t+1} - \bar{R}_t$ errors (starting from $\mathsf{A}$):

\[
    - \frac{1}{3}, - \frac{1}{3}, \frac{2}{3}, - \frac{1}{3}, - \frac{1}{3}, \frac{2}{3}, \dots
\]

Sequence of $\delta_t$ errors (starting from $\mathsf{A}$):
\[
    - \frac{1}{3} + 0 + \frac{1}{3}, - \frac{1}{3} + \frac{1}{3} - 0, \frac{2}{3} - \frac{1}{3} - \frac{1}{3}, \dots = 0,0,0,0,0,0, \dots
\]

The sequence of $\delta_t$ errors would produce a more stable estimate of the average reward, as it would always be zero.

\subsection*{Exercise 10.9}

In the differential semi-gradient $n$-step Sarsa algorithm, the step-size
parameter on the average reward, $\beta$, needs to be quite small so that $\bar{R}$ becomes a good
long-term estimate of the average reward. Unfortunately, $\bar{R}$ will then be biased by its
initial value for many steps, which may make learning inefficient. Alternatively, one could
use a sample average of the observed rewards for $\bar{R}$. That would initially adapt rapidly
but in the long run would also adapt slowly. As the policy slowly changed, $\bar{R}$ would also
change; the potential for such long-term nonstationarity makes sample-average methods
ill-suited. In fact, the step-size parameter on the average reward is a perfect place to use
the unbiased constant-step-size trick from Exercise 2.7. Describe the specific changes
needed to the boxed algorithm for differential semi-gradient $n$-step Sarsa to use this
trick.

\subsection*{Solution}
\fbox{
\begin{minipage}{0.95\textwidth}
    \footnotesize 

    Input: a differentiable function $\hat{q} : \mathcal{S} \times \mathcal{A} \times \mathbb{R}^d \to \mathbb{R}$, a policy $\pi$

    Initialize value-function weights $\mathbf{w} \in \mathbb{R}^d$ arbitrarily (e.g., $\mathbf{w} = \mathbf{0}$)

    Initialize average-reward estimate $\bar{R} \in \mathbb{R}$ arbitrarily (e.g., $\bar{R} = 0$)

    Algorithm parameters: step sizes $\alpha, \beta > 0$, small $\epsilon > 0$, a positive integer $n$

    All store and access operations ($S_t$, $A_t$, and $R_t$) can take their index mod $n+1$

    Initialize and store $S_0$ and $A_0$

    \textcolor{blue}{$\bar{o} \leftarrow 0$}

    Loop for each step, $t = 0, 1, 2, \dots$:
    \begin{itemize}[left=0em]
        \item[] Take action $A_t$
        \item[] Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$
        \item[] Select and store an action $A_{t+1} \sim \pi(\cdot | S_{t+1})$, or $\epsilon$-greedy with respect to $\hat{q}(S_{t+1}, \cdot, \mathbf{w})$
        \item[] $\tau \leftarrow t - n + 1$ \hspace{1cm} ($\tau$ is the time whose estimate is being updated)
        \item[] If $\tau \geq 0$:
        \begin{itemize}[left=0em]
            \item[] $\delta \leftarrow \sum_{i=\tau+1}^{\tau+n} (R_i - \bar{R}) + \hat{q}(S_{\tau+n}, A_{\tau+n}, \mathbf{w}) - \hat{q}(S_{\tau}, A_{\tau}, \mathbf{w})$
            \item[] \textcolor{blue}{$\bar{o} \leftarrow \bar{o} + \beta(1-\bar{o})$}
            \item[] \textcolor{blue}{$\bar{R} \leftarrow \bar{R} + \frac{\beta}{\bar{o}}\delta$}
            \item[] $\mathbf{w} \leftarrow \mathbf{w} + \alpha \delta \nabla \hat{q}(S_{\tau}, A_{\tau}, \mathbf{w})$
        \end{itemize}
    \end{itemize}
\end{minipage}
}