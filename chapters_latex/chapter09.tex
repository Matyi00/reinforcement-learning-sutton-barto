\section*{Chapter 9}

\subsection*{Exercise 9.1}

Show that tabular methods such as presented in Part I of this book are a special case of linear function approximation. What would the feature vectors be?

\subsection*{Solution}

Tabular methods are a special case of linear function approximation where each state is represented by a one-hot feature vector $\mathbf{x}(s)$. For a state $s_i$ in a space of $n$ states, $\mathbf{x}(s_i)$ is an $n$-dimensional vector with a $1$ at the $i$-th position and $0$ elsewhere. The value function is then:

\[
\hat{v}(s, \mathbf{w}) = \mathbf{x}(s)^\top \mathbf{w},
\]

where $\mathbf{w}$ is the vector of state values.

The SGD update rule then becomes:

\begin{align*}
    \mathbf{w}_{t+1} &= \mathbf{w}_t + \alpha \left[ U_t - \hat{v}(S_t, \mathbf{w}_t) \right] \nabla \hat{v}(S_t, \mathbf{w}_t) \\
    &= \mathbf{w}_t + \alpha \left[ U_t - \mathbf{x}(S_t)^\top \mathbf{w}_t \right] \mathbf{x}(S_t) \\
    &= \mathbf{w}_t + \alpha \left[ U_t - \mathbf{w}_t^{(S_t)}  \right] e_{S_t} \\
\end{align*}

Which is analogous to the tabular update rule:
\[
V_{t+1}(S_t) = V_t(S_t) + \alpha \left[ U_t - V_t(S_t) \right]
\]



\subsection*{Exercise 9.2}

Why does (9.17) define $(n + 1)^k$ distinct features for dimension $k$?

\subsection*{Solution}

Each of the $k$ dimensions independently contributes $(n+1)$ choices for the degree of the polynomial, so the total unique combinations of features is $(n+1)^k$.

\subsection*{Exercise 9.3}

What $n$ and $c_{i,j}$ produce the feature vectors $\mathbf{x}(s) = (1, s_1, s_2, s_1s_2, s_1^2, s_2^2, s_1s_2^2, s_1^2 s_2, s_1^2 s_2^2)^\top$?

\subsection*{Solution}

There are 2 states, and the maximum degree of the features is 2, so both $n$ and $k$ are 2. The coefficients $c_{i,j}$ are:
\[
C = \begin{bmatrix}
    0 & 0 \\
    1 & 0 \\
    0 & 1 \\
    1 & 1 \\
    2 & 0 \\
    0 & 2 \\
    1 & 2 \\
    2 & 1 \\
    2 & 2 \\
\end{bmatrix}
\]

\subsection*{Exercise 9.4}

Suppose we believe that one of two state dimensions is more likely to have
an effect on the value function than is the other, that generalization should be primarily
across this dimension rather than along it. What kind of tilings could be used to take
advantage of this prior knowledge?

\subsection*{Solution}

We need to use a tiling that is similar to the Asymmetric generalization in Figure 9.7. The tiling should have more tiles along the dimension that is more likely to have an effect on the value function.

\subsection*{Exercise 9.5}
Suppose you are using tile coding to transform a seven-dimensional continuous
state space into binary feature vectors to estimate a state value function $\hat{v}(s, \mathbf{w}) \approx v_\pi(s)$.
You believe that the dimensions do not interact strongly, so you decide to use eight tilings
of each dimension separately (stripe tilings), for $7 \times 8 = 56$ tilings. In addition, in case
there are some pairwise interactions between the dimensions, you also take all  ${{7}\choose{2}} = 21$
pairs of dimensions and tile each pair conjunctively with rectangular tiles. You make
two tilings for each pair of dimensions, making a grand total of $21 \times 2 + 56 = 98$ tilings.
Given these feature vectors, you suspect that you still have to average out some noise,
so you decide that you want learning to be gradual, taking about 10 presentations with
the same feature vector before learning nears its asymptote. What step-size parameter $\alpha$
should you use? Why?

\subsection*{Solution}
The feature vectors always have 98 1s, because each tiling only has exactly one tile activated, so $\mathbf{x}^\top \mathbf{x}$ is always 98. Thus $\alpha = \left(\tau \times \mathbb{E}[\mathbf{x}^\top \mathbf{x}]\right)^{-1} = \frac{1}{10 \times 98} = \frac{1}{980}$.

\subsection*{Exercise 9.6}
If $\tau = 1$ and $\mathbf{x}(S_t)^\top \mathbf{x}(S_t) = \mathbb{E}[\mathbf{x}^\top \mathbf{x}]$, prove that (9.19) together with (9.7)
and linear function approximation results in the error being reduced to zero in one update.

\subsection*{Solution}
Based on (9.19):
\[
    \alpha = \left( \tau \times \mathbb{E}[\mathbf{x}^\top \mathbf{x}] \right)^{-1} = \left( 1 \times \mathbf{x}(S_t)^\top \mathbf{x}(S_t) \right)^{-1} = \frac{1}{\mathbf{x}(S_t)^\top \mathbf{x}(S_t)}
\]

The update rule (9.7) for linear function approximation: 
\[
    \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \left[ U_t - \mathbf{x}(S_t)^\top \mathbf{w}_t \right] \mathbf{x}(S_t)
\]

The output for the new weight:
\begin{align*}
    \mathbf{w}_{t+1}^\top \mathbf{x}(S_t) &= \mathbf{w}_t^\top \mathbf{x}(S_t) + \alpha \left[ U_t - \mathbf{x}(S_t)^\top \mathbf{w}_t \right] \mathbf{x}(S_t)^\top \mathbf{x}(S_t) \\
    &= \mathbf{w}_t^\top \mathbf{x}(S_t) + \frac{1}{\mathbf{x}(S_t)^\top \mathbf{x}(S_t)} \left[ U_t - \mathbf{x}(S_t)^\top \mathbf{w}_t \right] \mathbf{x}(S_t)^\top \mathbf{x}(S_t)  \\
    &= \mathbf{w}_t^\top \mathbf{x}(S_t) + U_t - \mathbf{x}(S_t)^\top \mathbf{w}_t \\
    &= U_t
\end{align*}


\subsection*{Exercise 9.7}

One of the simplest artificial neural networks consists of a single semi-linear
unit with a logistic nonlinearity. The need to handle approximate value functions of this
form is common in games that end with either a win or a loss, in which case the value of
a state can be interpreted as the probability of winning. Derive the learning algorithm
for this case, from (9.7), such that no gradient notation appears.

\subsection*{Solution}


The logistic function is defined as:
\[
    \sigma(x) = \frac{1}{1 + e^{-x}}
\]

The derivative of the logistic function is:
\[
    \sigma'(x) = \sigma(x) (1 - \sigma(x))
\]

Using this, we can derive the learning algorithm for a single semi-linear unit with a logistic nonlinearity. The value function is:
\[
    \hat{v}(s, \mathbf{w}) = \sigma(\mathbf{x}(s)^\top \mathbf{w})
\]

The update rule (9.7) for linear function approximation is:
\[
    \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \left[ U_t - \hat{v}(S_t, \mathbf{w}_t) \right] \nabla \hat{v}(S_t, \mathbf{w}_t)
\]

Substituting the logistic function and its derivative:
\[
    \nabla \hat{v}(S_t, \mathbf{w}_t) = \sigma'(\mathbf{x}(S_t)^\top \mathbf{w}_t) \mathbf{x}(S_t) = \sigma(\mathbf{x}(S_t)^\top \mathbf{w}_t) (1 - \sigma(\mathbf{x}(S_t)^\top \mathbf{w}_t)) \mathbf{x}(S_t)
\]

Thus, the update rule becomes:
\[
    \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \left[ U_t - \sigma(\mathbf{x}(S_t)^\top \mathbf{w}_t) \right] \sigma(\mathbf{x}(S_t)^\top \mathbf{w}_t) (1 - \sigma(\mathbf{x}(S_t)^\top \mathbf{w}_t)) \mathbf{x}(S_t)
\]

Substituting $\hat{v}$, we get:
\[
    \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \left[ U_t - \hat{v}(S_t, \mathbf{w}_t) \right] \hat{v}(S_t, \mathbf{w}_t) (1 - \hat{v}(S_t, \mathbf{w}_t)) \mathbf{x}(S_t)
\]


\subsection*{*Exercise 9.8}
Arguably, the squared error used to derive (9.7) is inappropriate for the
case treated in the preceding exercise, and the right error measure is the \textit{cross-entropy
loss} (which you can find on Wikipedia). Repeat the derivation in Section 9.3, using the
cross-entropy loss instead of the squared error in (9.4), all the way to an explicit form
with no gradient or logarithm notation in it. Is your final form more complex, or simpler,
than that you obtained in the preceding exercise?

\subsection*{Solution}

The cross-entropy loss for a binary classification problem is defined as:
\[ 
    L(U_t, \hat{v}(S_t, \mathbf{w})) = - U_t \log(\hat{v}(S_t, \mathbf{w})) - (1 - U_t) \log(1 - \hat{v}(S_t, \mathbf{w}))
\]

The gradient of the cross-entropy loss with respect to the weights is:
\[ 
    \nabla L(U_t, \hat{v}(S_t, \mathbf{w})) = - \frac{U_t}{\hat{v}(S_t, \mathbf{w})} \nabla \hat{v}(S_t, \mathbf{w}) + \frac{1 - U_t}{1 - \hat{v}(S_t, \mathbf{w})} \nabla \hat{v}(S_t, \mathbf{w})
\]

Using the logistic function and its derivative:
\[ 
    \nabla \hat{v}(S_t, \mathbf{w}) = \hat{v}(S_t, \mathbf{w}) (1 - \hat{v}(S_t, \mathbf{w})) \mathbf{x}(S_t)
\]

Substituting this into the gradient of the cross-entropy loss:
\begin{align*} 
    \nabla L(U_t, \hat{v}(S_t, \mathbf{w})) = &- \frac{U_t}{\hat{v}(S_t, \mathbf{w})} \hat{v}(S_t, \mathbf{w}) (1 - \hat{v}(S_t, \mathbf{w})) \mathbf{x}(S_t) \\
    &+ \frac{1 - U_t}{1 - \hat{v}(S_t, \mathbf{w})} \hat{v}(S_t, \mathbf{w}) (1 - \hat{v}(S_t, \mathbf{w})) \mathbf{x}(S_t)
\end{align*}

Simplifying:
\[ 
    \nabla L(U_t, \hat{v}(S_t, \mathbf{w})) = - U_t (1 - \hat{v}(S_t, \mathbf{w})) \mathbf{x}(S_t) + (1 - U_t) \hat{v}(S_t, \mathbf{w}) \mathbf{x}(S_t)
\]
\[ 
    \nabla L(U_t, \hat{v}(S_t, \mathbf{w})) = (\hat{v}(S_t, \mathbf{w}) - U_t) \mathbf{x}(S_t)
\]

The update rule for the weights using the cross-entropy loss is:
\begin{align*}
    \mathbf{w}_{t+1} &= \mathbf{w}_t - \alpha \nabla L(U_t, \hat{v}(S_t, \mathbf{w}_t)) \\
    \mathbf{w}_{t+1} &= \mathbf{w}_t - \alpha (\hat{v}(S_t, \mathbf{w}_t) - U_t) \mathbf{x}(S_t)
\end{align*}

Thus, the final update rule is:
\[ 
    \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha (U_t - \hat{v}(S_t, \mathbf{w}_t)) \mathbf{x}(S_t)
\]

This form is simpler than the one obtained in the preceding exercise.